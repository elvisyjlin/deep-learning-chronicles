{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Requests import Requests\n",
    "from utils.helpers import parse_dates\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def get_arxiv(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [a.text for a in soup.find('div', {'class': 'authors'}).find_all('a')]\n",
    "    dates = soup.find('div', {'class': 'dateline'}).text.split(' (v1), ')\n",
    "    dates = [date.replace('(Submitted on ', '').replace('last revised ', '').split(' (')[0].strip(' ()') for date in dates]\n",
    "    dates = parse_dates(dates)\n",
    "    abstract = soup.find('blockquote', {'class': 'abstract mathjax'}).text.replace('Abstract: ', '').replace('\\n', ' ').strip()\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_acm(url):\n",
    "    r = Requests()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Safari/605.1.15'}\n",
    "    text = r.get(url, params={'headers': headers})\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').text\n",
    "    authors = [a.text for a in soup.find_all('table')[4].find('table').find_all('table')[1].find_all('a')[::2]]\n",
    "    tmp = soup.find_all(lambda tag: tag.name == \"td\" and \"—\" in tag.text)\n",
    "    if tmp != []:\n",
    "        date_tokens = tmp[-1].text.split('\\n')[3].split(' — ')[1].split()\n",
    "        dates = parse_dates([' '.join([date_tokens[0], date_tokens[1], date_tokens[4]])])\n",
    "    else:\n",
    "        tmp = soup.find_all(lambda tag: tag.name == \"td\" and \"©\" in tag.text)\n",
    "        if tmp != []:\n",
    "            year = tmp[-1].text.split('\\n')[2].split('©')[1]\n",
    "            dates = parse_dates([year], reset=True)\n",
    "        else:\n",
    "            tmp = soup.find_all(lambda tag: tag.name == \"td\" and \"Article\" in tag.text)\n",
    "            year = tmp[0].text.strip().split(' ')[0]\n",
    "            dates = parse_dates([year], reset=True)\n",
    "    abstract = r.get(\n",
    "        'https://dl.acm.org/tab_abstract.cfm?id={}&type=Article&usebody=tabbody&_cf_containerId=cf_layoutareaabstract&_cf_nodebug=true&_cf_nocache=true&_cf_rc=0'.format(url.split('=')[1]), \n",
    "        params={'headers': headers}\n",
    "    )\n",
    "    abstract = BS(abstract, 'html.parser').find('p').text\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_acl(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_publication_date'})['content']], reset=True)\n",
    "    abstract = ''\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_scholarpedia(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_date'})['content']])\n",
    "    abstract = ''\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_nature(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_online_date'})['content']])\n",
    "    abstract = soup.find('div', {'id': 'abstract-content'}).find('p').text\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_empty(url):\n",
    "    result = {\n",
    "        'title': '', \n",
    "        'authors': '', \n",
    "        'dates': '',\n",
    "        'abstract': ''\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# r = Requests()\n",
    "# url = 'https://arxiv.org/abs/1612.03242'\n",
    "# text = r.get(url)\n",
    "# soup = BS(text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DTIC</td>\n",
       "      <td>http://www.dtic.mil/docs/citations/AD0256582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Backpropagation</td>\n",
       "      <td>DTIC</td>\n",
       "      <td>http://www.dtic.mil/docs/citations/ADA164453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Linearity</td>\n",
       "      <td>MCSS</td>\n",
       "      <td>https://link.springer.com/article/10.1007/BF02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Activation Functions</td>\n",
       "      <td>Book</td>\n",
       "      <td>https://dl.acm.org/citation.cfm?id=541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBM</td>\n",
       "      <td>ICML 2007</td>\n",
       "      <td>https://dl.acm.org/citation.cfm?id=1273596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name     source  \\\n",
       "0                   MLP       DTIC   \n",
       "1       Backpropagation       DTIC   \n",
       "2         Non-Linearity       MCSS   \n",
       "3  Activation Functions       Book   \n",
       "4                   RBM  ICML 2007   \n",
       "\n",
       "                                                 url  \n",
       "0       http://www.dtic.mil/docs/citations/AD0256582  \n",
       "1       http://www.dtic.mil/docs/citations/ADA164453  \n",
       "2  https://link.springer.com/article/10.1007/BF02...  \n",
       "3          https://dl.acm.org/citation.cfm?id=541500  \n",
       "4         https://dl.acm.org/citation.cfm?id=1273596  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized url: http://www.dtic.mil/docs/citations/AD0256582\n",
      "Unrecognized url: http://www.dtic.mil/docs/citations/ADA164453\n",
      "Unrecognized url: https://link.springer.com/article/10.1007/BF02551274\n",
      "Unrecognized url: https://www.researchgate.net/publication/269295652_Deep_neural_networks_for_small_footprint_text-dependent_speaker_verification\n",
      "Unrecognized url: https://www.researchgate.net/publication/327812023_X-Vectors_Robust_DNN_Embeddings_for_Speaker_Recognition\n",
      "Unrecognized url: https://ieeexplore.ieee.org/document/7780518/\n",
      "Unrecognized url: https://ieeexplore.ieee.org/document/8237776/\n",
      "Unrecognized url: https://deepmind.com/blog/neural-scene-representation-and-rendering/\n",
      "Unrecognized url: https://openreview.net/forum?id=SkfMWhAqYQ\n",
      "Unrecognized url: ''\n",
      "Unrecognized url: https://github.com/facebookresearch/Detectron\n",
      "Unrecognized url: https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf\n",
      "[{'name': 'Activation Functions', 'source': 'Book', 'url': 'https://dl.acm.org/citation.cfm?id=541500', 'title': 'Neural Networks', 'authors': ['Simon Haykin'], 'dates': ['1994-01-01'], 'abstract': 'From the Publisher:'}, {'name': 'RBM', 'source': 'ICML 2007', 'url': 'https://dl.acm.org/citation.cfm?id=1273596', 'title': 'Restricted Boltzmann machines for collaborative filtering', 'authors': ['Ruslan Salakhutdinov', 'Andriy Mnih', 'Geoffrey Hinton'], 'dates': ['2007-06-20'], 'abstract': \"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.\"}, {'name': 'Denoising Auto-Encoder', 'source': 'ICML 2008', 'url': 'https://dl.acm.org/citation.cfm?id=1390294', 'title': 'Extracting and composing robust features with denoising autoencoders', 'authors': ['Pascal Vincent', 'Hugo Larochelle', 'Yoshua Bengio', 'Pierre-Antoine Manzagol'], 'dates': ['2008-07-05'], 'abstract': 'Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.'}, {'name': 'DBN', 'source': 'Scholarpedia', 'url': 'http://scholarpedia.org/article/Deep_belief_networks', 'title': 'Deep belief networks', 'authors': ['Geoffrey E. Hinton'], 'dates': ['2009-05-31'], 'abstract': ''}, {'name': 'AlexNet', 'source': 'NIPS 2012', 'url': 'https://dl.acm.org/citation.cfm?id=2999257', 'title': 'ImageNet classification with deep convolutional neural networks', 'authors': ['Alex Krizhevsky', 'Ilya Sutskever', 'Geoffrey E. Hinton'], 'dates': ['2012-12-03'], 'abstract': 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.'}]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    info = {\n",
    "        'name': row['name'], \n",
    "        'source': row['source'], \n",
    "        'url': row['url']\n",
    "    }\n",
    "    try:\n",
    "        if 'arxiv' in url:\n",
    "            info.update(get_arxiv(url))\n",
    "        elif 'acm' in url:\n",
    "            info.update(get_acm(url))\n",
    "        elif 'acl' in url:\n",
    "            info.update(get_acl(url))\n",
    "        elif 'scholarpedia' in url:\n",
    "            info.update(get_scholarpedia(url))\n",
    "        elif 'nature' in url:\n",
    "            info.update(get_nature(url))\n",
    "        else:\n",
    "            print('Unrecognized url:', url)\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        raise e\n",
    "    result.append(info)\n",
    "print(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(result, open('works.json', 'w', encoding='utf-8'), sort_keys=True, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
