[
  {
    "abstract": "From the Publisher:",
    "authors": [
      "Simon Haykin"
    ],
    "dates": [
      "1994-01-01"
    ],
    "name": "Activation Functions",
    "source": "Book",
    "title": "Neural Networks",
    "url": "https://dl.acm.org/citation.cfm?id=541500"
  },
  {
    "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",
    "authors": [
      "Ruslan Salakhutdinov",
      "Andriy Mnih",
      "Geoffrey Hinton"
    ],
    "dates": [
      "2007-06-20"
    ],
    "name": "RBM",
    "source": "ICML 2007",
    "title": "Restricted Boltzmann machines for collaborative filtering",
    "url": "https://dl.acm.org/citation.cfm?id=1273596"
  },
  {
    "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",
    "authors": [
      "Pascal Vincent",
      "Hugo Larochelle",
      "Yoshua Bengio",
      "Pierre-Antoine Manzagol"
    ],
    "dates": [
      "2008-07-05"
    ],
    "name": "Denoising Auto-Encoder",
    "source": "ICML 2008",
    "title": "Extracting and composing robust features with denoising autoencoders",
    "url": "https://dl.acm.org/citation.cfm?id=1390294"
  },
  {
    "abstract": "",
    "authors": [
      "Geoffrey E. Hinton"
    ],
    "dates": [
      "2009-05-31"
    ],
    "name": "DBN",
    "source": "Scholarpedia",
    "title": "Deep belief networks",
    "url": "http://scholarpedia.org/article/Deep_belief_networks"
  },
  {
    "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
    "authors": [
      "Alex Krizhevsky",
      "Ilya Sutskever",
      "Geoffrey E. Hinton"
    ],
    "dates": [
      "2012-12-03"
    ],
    "name": "AlexNet",
    "source": "NIPS 2012",
    "title": "ImageNet classification with deep convolutional neural networks",
    "url": "https://dl.acm.org/citation.cfm?id=2999257"
  },
  {
    "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
    "authors": [
      "Volodymyr Mnih",
      "Koray Kavukcuoglu",
      "David Silver",
      "Alex Graves",
      "Ioannis Antonoglou",
      "Daan Wierstra",
      "Martin Riedmiller"
    ],
    "dates": [
      "2013-12-19"
    ],
    "name": "DQN",
    "source": "NIPS Deep Learning Workshop 2013",
    "title": "Playing Atari with Deep Reinforcement Learning",
    "url": "https://arxiv.org/abs/1312.5602"
  },
  {
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
    "authors": [
      "Tomas Mikolov",
      "Kai Chen",
      "Greg Corrado",
      "Jeffrey Dean"
    ],
    "dates": [
      "2013-01-16",
      "2013-09-07"
    ],
    "name": "Word2Vec (CBOW)",
    "source": "NIPS 2013",
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "url": "https://arxiv.org/abs/1301.3781"
  },
  {
    "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "authors": [
      "Tomas Mikolov",
      "Ilya Sutskever",
      "Kai Chen",
      "Greg Corrado",
      "Jeffrey Dean"
    ],
    "dates": [
      "2013-10-16"
    ],
    "name": "Word2Vec (Skip-Gram)",
    "source": "NIPS 2013",
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "url": "https://arxiv.org/abs/1310.4546"
  },
  {
    "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",
    "authors": [
      "Yoshua Bengio",
      "Aaron Courville",
      "Pascal Vincent"
    ],
    "dates": [
      "2012-06-24",
      "2014-04-23"
    ],
    "name": "Deep Auto-Encoder",
    "source": "TPAMI 2013",
    "title": "Representation Learning: A Review and New Perspectives",
    "url": "https://arxiv.org/abs/1206.5538"
  },
  {
    "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
    "authors": [
      "Olga Russakovsky",
      "Jia Deng",
      "Hao Su",
      "Jonathan Krause",
      "Sanjeev Satheesh",
      "Sean Ma",
      "Zhiheng Huang",
      "Andrej Karpathy",
      "Aditya Khosla",
      "Michael Bernstein",
      "Alexander C. Berg",
      "Li Fei-Fei"
    ],
    "dates": [
      "2014-09-01",
      "2015-01-30"
    ],
    "name": "ImageNet",
    "source": "Arxiv",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "url": "https://arxiv.org/abs/1409.0575"
  },
  {
    "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
    "authors": [
      "Ian J. Goodfellow",
      "Jean Pouget-Abadie",
      "Mehdi Mirza",
      "Bing Xu",
      "David Warde-Farley",
      "Sherjil Ozair",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "dates": [
      "2014-06-10"
    ],
    "name": "GAN",
    "source": "NIPS 2014",
    "title": "Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1406.2661"
  },
  {
    "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "authors": [
      "Diederik P Kingma",
      "Max Welling"
    ],
    "dates": [
      "2013-12-20",
      "2014-05-01"
    ],
    "name": "VAE",
    "source": "ICLR 2014",
    "title": "Auto-Encoding Variational Bayes",
    "url": "https://arxiv.org/abs/1312.6114"
  },
  {
    "abstract": "",
    "authors": [
      "Jeffrey Pennington",
      "Richard Socher",
      "Christopher Manning"
    ],
    "dates": [
      "2014-01-01"
    ],
    "name": "GloVe",
    "source": "EMNLP 2014",
    "title": "Glove: Global Vectors for Word Representation",
    "url": "https://aclanthology.coli.uni-saarland.de/papers/D14-1162/d14-1162"
  },
  {
    "abstract": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.",
    "authors": [
      "Matthew Hausknecht",
      "Peter Stone"
    ],
    "dates": [
      "2015-07-23",
      "2017-01-11"
    ],
    "name": "DRQN",
    "source": "AAAI 2015",
    "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
    "url": "https://arxiv.org/abs/1507.06527"
  },
  {
    "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
    "authors": [
      "Olaf Ronneberger",
      "Philipp Fischer",
      "Thomas Brox"
    ],
    "dates": [
      "2015-05-18"
    ],
    "name": "U-Net",
    "source": "MICCAI 2015",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "url": "https://arxiv.org/abs/1505.04597"
  },
  {
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
    "authors": [
      "Karen Simonyan",
      "Andrew Zisserman"
    ],
    "dates": [
      "2014-09-04",
      "2015-04-10"
    ],
    "name": "VGGNet",
    "source": "ICLR 2015",
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "url": "https://arxiv.org/abs/1409.1556"
  },
  {
    "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
    "authors": [
      "Christian Szegedy",
      "Wei Liu",
      "Yangqing Jia",
      "Pierre Sermanet",
      "Scott Reed",
      "Dragomir Anguelov",
      "Dumitru Erhan",
      "Vincent Vanhoucke",
      "Andrew Rabinovich"
    ],
    "dates": [
      "2014-09-17"
    ],
    "name": "Inception-v1",
    "source": "CVPR 2015",
    "title": "Going Deeper with Convolutions",
    "url": "https://arxiv.org/abs/1409.4842"
  },
  {
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
    "authors": [
      "Sergey Ioffe",
      "Christian Szegedy"
    ],
    "dates": [
      "2015-02-11",
      "2015-03-02"
    ],
    "name": "Batch Normalization",
    "source": "Arxiv",
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "url": "https://arxiv.org/abs/1502.03167"
  },
  {
    "abstract": "In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.",
    "authors": [
      "Alireza Makhzani",
      "Jonathon Shlens",
      "Navdeep Jaitly",
      "Ian Goodfellow",
      "Brendan Frey"
    ],
    "dates": [
      "2015-11-18",
      "2016-05-25"
    ],
    "name": "Adversarial Autoencoders",
    "source": "Arxiv",
    "title": "Adversarial Autoencoders",
    "url": "https://arxiv.org/abs/1511.05644"
  },
  {
    "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
    "authors": [
      "Ziyu Wang",
      "Tom Schaul",
      "Matteo Hessel",
      "Hado van Hasselt",
      "Marc Lanctot",
      "Nando de Freitas"
    ],
    "dates": [
      "2015-11-20",
      "2016-04-05"
    ],
    "name": "Dualing Network",
    "source": "Arxiv",
    "title": "Dueling Network Architectures for Deep Reinforcement Learning",
    "url": "https://arxiv.org/abs/1511.06581"
  },
  {
    "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.",
    "authors": [
      "Oriol Vinyals",
      "Meire Fortunato",
      "Navdeep Jaitly"
    ],
    "dates": [
      "2015-06-09",
      "2017-01-02"
    ],
    "name": "Pointer Networks",
    "source": "NIPS 2015",
    "title": "Pointer Networks",
    "url": "https://arxiv.org/abs/1506.03134"
  },
  {
    "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
    "authors": [
      "Jonathan Long",
      "Evan Shelhamer",
      "Trevor Darrell"
    ],
    "dates": [
      "2014-11-14",
      "2015-03-08"
    ],
    "name": "FCN-8s,16s,32s",
    "source": "CVPR 2015",
    "title": "Fully Convolutional Networks for Semantic Segmentation",
    "url": "https://arxiv.org/abs/1411.4038"
  },
  {
    "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
    "authors": [
      "Armand Joulin",
      "Edouard Grave",
      "Piotr Bojanowski",
      "Tomas Mikolov"
    ],
    "dates": [
      "2016-07-06",
      "2016-08-09"
    ],
    "name": "fastText",
    "source": "Arxiv",
    "title": "Bag of Tricks for Efficient Text Classification",
    "url": "https://arxiv.org/abs/1607.01759"
  },
  {
    "abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.",
    "authors": [
      "Hado van Hasselt",
      "Arthur Guez",
      "David Silver"
    ],
    "dates": [
      "2015-09-22",
      "2015-12-08"
    ],
    "name": "Double DQN",
    "source": "AAAI 2016",
    "title": "Deep Reinforcement Learning with Double Q-learning",
    "url": "https://arxiv.org/abs/1509.06461"
  },
  {
    "abstract": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
    "authors": [
      "Junbo Zhao",
      "Michael Mathieu",
      "Ross Goroshin",
      "Yann LeCun"
    ],
    "dates": [
      "2015-06-08",
      "2016-02-14"
    ],
    "name": "SWWAE",
    "source": "ICLR 2016",
    "title": "Stacked What-Where Auto-encoders",
    "url": "https://arxiv.org/abs/1506.02351"
  },
  {
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
    "authors": [
      "Alec Radford",
      "Luke Metz",
      "Soumith Chintala"
    ],
    "dates": [
      "2015-11-19",
      "2016-01-07"
    ],
    "name": "DCGAN",
    "source": "ICLR 2016",
    "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1511.06434"
  },
  {
    "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
    "authors": [
      "Scott Reed",
      "Zeynep Akata",
      "Xinchen Yan",
      "Lajanugen Logeswaran",
      "Bernt Schiele",
      "Honglak Lee"
    ],
    "dates": [
      "2016-05-17",
      "2016-06-05"
    ],
    "name": "text2image",
    "source": "ICML 2016",
    "title": "Generative Adversarial Text to Image Synthesis",
    "url": "https://arxiv.org/abs/1605.05396"
  },
  {
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
    "authors": [
      "Timothy P. Lillicrap",
      "Jonathan J. Hunt",
      "Alexander Pritzel",
      "Nicolas Heess",
      "Tom Erez",
      "Yuval Tassa",
      "David Silver",
      "Daan Wierstra"
    ],
    "dates": [
      "2015-09-09",
      "2016-02-29"
    ],
    "name": "DDPG",
    "source": "ICML 2016",
    "title": "Continuous control with deep reinforcement learning",
    "url": "https://arxiv.org/abs/1509.02971"
  },
  {
    "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
    "authors": [
      "Volodymyr Mnih",
      "Adri\u00e0 Puigdom\u00e8nech Badia",
      "Mehdi Mirza",
      "Alex Graves",
      "Timothy P. Lillicrap",
      "Tim Harley",
      "David Silver",
      "Koray Kavukcuoglu"
    ],
    "dates": [
      "2016-02-04",
      "2016-06-16"
    ],
    "name": "A3C",
    "source": "ICML 2016",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "url": "https://arxiv.org/abs/1602.01783"
  },
  {
    "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.",
    "authors": [
      "Xi Chen",
      "Yan Duan",
      "Rein Houthooft",
      "John Schulman",
      "Ilya Sutskever",
      "Pieter Abbeel"
    ],
    "dates": [
      "2016-06-12"
    ],
    "name": "Infogan",
    "source": "NIPS 2016",
    "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
    "url": "https://arxiv.org/abs/1606.03657"
  },
  {
    "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
    "authors": [
      "Kaiming He",
      "Xiangyu Zhang",
      "Shaoqing Ren",
      "Jian Sun"
    ],
    "dates": [
      "2015-12-10"
    ],
    "name": "ResNet",
    "source": "CVPR 2016",
    "title": "Deep Residual Learning for Image Recognition",
    "url": "https://arxiv.org/abs/1512.03385"
  },
  {
    "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
    "authors": [
      "Christian Szegedy",
      "Vincent Vanhoucke",
      "Sergey Ioffe",
      "Jonathon Shlens",
      "Zbigniew Wojna"
    ],
    "dates": [
      "2015-12-02",
      "2015-12-11"
    ],
    "name": "Inception-v2,v3",
    "source": "CVPR 2016",
    "title": "Rethinking the Inception Architecture for Computer Vision",
    "url": "https://arxiv.org/abs/1512.00567"
  },
  {
    "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at this https URL. Full paper can be found at arXiv:1701.02096.",
    "authors": [
      "Dmitry Ulyanov",
      "Andrea Vedaldi",
      "Victor Lempitsky"
    ],
    "dates": [
      "2016-07-27",
      "2017-11-06"
    ],
    "name": "Instance Normalization",
    "source": "Arxiv",
    "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
    "url": "https://arxiv.org/abs/1607.08022"
  },
  {
    "abstract": "While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.",
    "authors": [
      "Xide Xia",
      "Brian Kulis"
    ],
    "dates": [
      "2017-11-22"
    ],
    "name": "W-Net",
    "source": "Arxiv",
    "title": "W-Net: A Deep Model for Fully Unsupervised Image Segmentation",
    "url": "https://arxiv.org/abs/1711.08506"
  },
  {
    "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
    "authors": [
      "Abigail See",
      "Peter J. Liu",
      "Christopher D. Manning"
    ],
    "dates": [
      "2017-04-14",
      "2017-04-25"
    ],
    "name": "Pointer-Generator Networks",
    "source": "ACL 2017",
    "title": "Get To The Point: Summarization with Pointer-Generator Networks",
    "url": "https://arxiv.org/abs/1704.04368"
  },
  {
    "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",
    "authors": [
      "Armand Joulin",
      "Edouard Grave",
      "Piotr Bojanowski",
      "Matthijs Douze",
      "H\u00e9rve J\u00e9gou",
      "Tomas Mikolov"
    ],
    "dates": [
      "2016-12-12"
    ],
    "name": "fastText",
    "source": "ICLR 2017",
    "title": "FastText.zip: Compressing text classification models",
    "url": "https://arxiv.org/abs/1612.03651"
  },
  {
    "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
    "authors": [
      "Piotr Bojanowski",
      "Edouard Grave",
      "Armand Joulin",
      "Tomas Mikolov"
    ],
    "dates": [
      "2016-07-15",
      "2017-06-19"
    ],
    "name": "fastText",
    "source": "TACL 2017",
    "title": "Enriching Word Vectors with Subword Information",
    "url": "https://arxiv.org/abs/1607.04606"
  },
  {
    "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.",
    "authors": [
      "Augustus Odena",
      "Christopher Olah",
      "Jonathon Shlens"
    ],
    "dates": [
      "2016-10-30",
      "2017-07-20"
    ],
    "name": "ACGAN",
    "source": "ICML 2017",
    "title": "Conditional Image Synthesis With Auxiliary Classifier GANs",
    "url": "https://arxiv.org/abs/1610.09585"
  },
  {
    "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\u03c7^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",
    "authors": [
      "Xudong Mao",
      "Qing Li",
      "Haoran Xie",
      "Raymond Y.K. Lau",
      "Zhen Wang",
      "Stephen Paul Smolley"
    ],
    "dates": [
      "2016-11-13",
      "2017-04-05"
    ],
    "name": "LSGAN",
    "source": "ICCV 2017",
    "title": "Least Squares Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1611.04076"
  },
  {
    "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. We also show that the derived objective function that yields minimizing the Pearson $\\chi^2$ divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method --- datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",
    "authors": [
      "Xudong Mao",
      "Qing Li",
      "Haoran Xie",
      "Raymond Y.K. Lau",
      "Zhen Wang",
      "Stephen Paul Smolley"
    ],
    "dates": [
      "2017-12-18",
      "2018-09-21"
    ],
    "name": "LSGAN-GP",
    "source": "TPAMI 2018",
    "title": "On the Effectiveness of Least Squares Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1712.06391"
  },
  {
    "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.",
    "authors": [
      "Martin Arjovsky",
      "Soumith Chintala",
      "L\u00e9on Bottou"
    ],
    "dates": [
      "2017-01-26",
      "2017-12-06"
    ],
    "name": "WGAN",
    "source": "Arxiv",
    "title": "Wasserstein GAN",
    "url": "https://arxiv.org/abs/1701.07875"
  },
  {
    "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
    "authors": [
      "Ishaan Gulrajani",
      "Faruk Ahmed",
      "Martin Arjovsky",
      "Vincent Dumoulin",
      "Aaron Courville"
    ],
    "dates": [
      "2017-03-31",
      "2017-12-25"
    ],
    "name": "WGAN-GP",
    "source": "NIPS 2017",
    "title": "Improved Training of Wasserstein GANs",
    "url": "https://arxiv.org/abs/1704.00028"
  },
  {
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "dates": [
      "2017-06-12",
      "2017-12-06"
    ],
    "name": "Transformer",
    "source": "NIPS 2017",
    "title": "Attention Is All You Need",
    "url": "https://arxiv.org/abs/1706.03762"
  },
  {
    "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
    "authors": [
      "Phillip Isola",
      "Jun-Yan Zhu",
      "Tinghui Zhou",
      "Alexei A. Efros"
    ],
    "dates": [
      "2016-11-21",
      "2018-11-26"
    ],
    "name": "pix2pix",
    "source": "CVPR 2017",
    "title": "Image-to-Image Translation with Conditional Adversarial Networks",
    "url": "https://arxiv.org/abs/1611.07004"
  },
  {
    "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
    "authors": [
      "Fran\u00e7ois Chollet"
    ],
    "dates": [
      "2016-10-07",
      "2017-04-04"
    ],
    "name": "Xception",
    "source": "CVPR 2017",
    "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
    "url": "https://arxiv.org/abs/1610.02357"
  },
  {
    "abstract": "We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.",
    "authors": [
      "David Berthelot",
      "Thomas Schumm",
      "Luke Metz"
    ],
    "dates": [
      "2017-03-31",
      "2017-05-31"
    ],
    "name": "BEGAN",
    "source": "Arxiv",
    "title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1703.10717"
  },
  {
    "abstract": "We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.",
    "authors": [
      "Naveen Kodali",
      "Jacob Abernethy",
      "James Hays",
      "Zsolt Kira"
    ],
    "dates": [
      "2017-05-19",
      "2017-12-10"
    ],
    "name": "DRAGAN",
    "source": "Arxiv",
    "title": "On Convergence and Stability of GANs",
    "url": "https://arxiv.org/abs/1705.07215"
  },
  {
    "abstract": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.",
    "authors": [
      "Sebastian Nowozin",
      "Botond Cseke",
      "Ryota Tomioka"
    ],
    "dates": [
      "2016-06-02"
    ],
    "name": "f-GAN",
    "source": "NIPS 2016",
    "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
    "url": "https://arxiv.org/abs/1606.00709"
  },
  {
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
    "authors": [
      "Takeru Miyato",
      "Toshiki Kataoka",
      "Masanori Koyama",
      "Yuichi Yoshida"
    ],
    "dates": [
      "2018-02-16"
    ],
    "name": "SNGAN",
    "source": "ICLR 2018",
    "title": "Spectral Normalization for Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1802.05957"
  },
  {
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.",
    "authors": [
      "Takeru Miyato",
      "Masanori Koyama"
    ],
    "dates": [
      "2018-02-15",
      "2018-08-15"
    ],
    "name": "Projection Discriminators",
    "source": "ICLR 2018",
    "title": "cGANs with Projection Discriminator",
    "url": "https://arxiv.org/abs/1802.05637"
  },
  {
    "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
    "authors": [
      "Han Zhang",
      "Ian Goodfellow",
      "Dimitris Metaxas",
      "Augustus Odena"
    ],
    "dates": [
      "2018-05-21"
    ],
    "name": "SAGAN",
    "source": "Arxiv",
    "title": "Self-Attention Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1805.08318"
  },
  {
    "abstract": "We know SGAN may have a risk of gradient vanishing. A significant improvement is WGAN, with the help of 1-Lipschitz constraint on discriminator to prevent from gradient vanishing. Is there any GAN having no gradient vanishing and no 1-Lipschitz constraint on discriminator? We do find one, called GAN-QP. To construct a new framework of Generative Adversarial Network (GAN) usually includes three steps: 1. choose a probability divergence; 2. convert it into a dual form; 3. play a min-max game. In this articles, we demonstrate that the first step is not necessary. We can analyse the property of divergence and even construct new divergence in dual space directly. As a reward, we obtain a simpler alternative of WGAN: GAN-QP. We demonstrate that GAN-QP have a better performance than WGAN in theory and practice.",
    "authors": [
      "Jianlin Su"
    ],
    "dates": [
      "2018-11-18",
      "2018-12-15"
    ],
    "name": "GAN-QP",
    "source": "Arxiv",
    "title": "GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint",
    "url": "https://arxiv.org/abs/1811.07296"
  },
  {
    "abstract": "While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available this https URL",
    "authors": [
      "Taeksoo Kim",
      "Moonsu Cha",
      "Hyunsoo Kim",
      "Jung Kwon Lee",
      "Jiwon Kim"
    ],
    "dates": [
      "2017-03-15",
      "2017-05-15"
    ],
    "name": "DiscoGAN",
    "source": "ICML 2017",
    "title": "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1703.05192"
  },
  {
    "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: this https URL",
    "authors": [
      "Forrest N. Iandola",
      "Song Han",
      "Matthew W. Moskewicz",
      "Khalid Ashraf",
      "William J. Dally",
      "Kurt Keutzer"
    ],
    "dates": [
      "2016-02-24",
      "2016-11-04"
    ],
    "name": "SqueezeNet",
    "source": "Arxiv",
    "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
    "url": "https://arxiv.org/abs/1602.07360"
  },
  {
    "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
    "authors": [
      "Jun-Yan Zhu",
      "Taesung Park",
      "Phillip Isola",
      "Alexei A. Efros"
    ],
    "dates": [
      "2017-03-30",
      "2018-11-15"
    ],
    "name": "CycleGAN",
    "source": "ICCV 2017",
    "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
    "url": "https://arxiv.org/abs/1703.10593"
  },
  {
    "abstract": "Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",
    "authors": [
      "Zili Yi",
      "Hao Zhang",
      "Ping Tan",
      "Minglun Gong"
    ],
    "dates": [
      "2017-04-08",
      "2018-10-09"
    ],
    "name": "DualGAN",
    "source": "ICCV 2017",
    "title": "DualGAN: Unsupervised Dual Learning for Image-to-Image Translation",
    "url": "https://arxiv.org/abs/1704.02510"
  },
  {
    "abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
    "authors": [
      "Han Zhang",
      "Tao Xu",
      "Hongsheng Li",
      "Shaoting Zhang",
      "Xiaogang Wang",
      "Xiaolei Huang",
      "Dimitris Metaxas"
    ],
    "dates": [
      "2016-12-10",
      "2017-08-05"
    ],
    "name": "StackGAN",
    "source": "ICCV 2017",
    "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1612.03242"
  },
  {
    "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge",
    "authors": [
      "Christian Szegedy",
      "Sergey Ioffe",
      "Vincent Vanhoucke",
      "Alex Alemi"
    ],
    "dates": [
      "2016-02-23",
      "2016-08-23"
    ],
    "name": "Inception-v4, Inception-ResNet",
    "source": "AAAI 2017",
    "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
    "url": "https://arxiv.org/abs/1602.07261"
  },
  {
    "abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",
    "authors": [
      "Lars Mescheder",
      "Sebastian Nowozin",
      "Andreas Geiger"
    ],
    "dates": [
      "2017-01-17",
      "2018-06-11"
    ],
    "name": "Adversarial Variational Bayes",
    "source": "Arxiv",
    "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1701.04722"
  },
  {
    "abstract": "The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.",
    "authors": [
      "Matteo Hessel",
      "Joseph Modayil",
      "Hado van Hasselt",
      "Tom Schaul",
      "Georg Ostrovski",
      "Will Dabney",
      "Dan Horgan",
      "Bilal Piot",
      "Mohammad Azar",
      "David Silver"
    ],
    "dates": [
      "2017-10-06"
    ],
    "name": "Rainbow",
    "source": "AAAI 2018",
    "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
    "url": "https://arxiv.org/abs/1710.02298"
  },
  {
    "abstract": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",
    "authors": [
      "Yunjey Choi",
      "Minje Choi",
      "Munyoung Kim",
      "Jung-Woo Ha",
      "Sunghun Kim",
      "Jaegul Choo"
    ],
    "dates": [
      "2017-11-24",
      "2018-09-21"
    ],
    "name": "StarGAN",
    "source": "CVPR 2018",
    "title": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation",
    "url": "https://arxiv.org/abs/1711.09020"
  },
  {
    "abstract": "In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.",
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ],
    "dates": [
      "2018-07-02",
      "2018-09-10"
    ],
    "name": "Relativistic GAN",
    "source": "Arxiv",
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "url": "https://arxiv.org/abs/1807.00734"
  },
  {
    "abstract": "Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.",
    "authors": [
      "Han Zhang",
      "Tao Xu",
      "Hongsheng Li",
      "Shaoting Zhang",
      "Xiaogang Wang",
      "Xiaolei Huang",
      "Dimitris Metaxas"
    ],
    "dates": [
      "2017-10-19",
      "2018-06-28"
    ],
    "name": "StackGAN++",
    "source": "TPAMI 2018",
    "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1710.10916"
  },
  {
    "abstract": "This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it (1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic-sounding speech. Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.",
    "authors": [
      "Hirokazu Kameoka",
      "Takuhiro Kaneko",
      "Kou Tanaka",
      "Nobukatsu Hojo"
    ],
    "dates": [
      "2018-06-06",
      "2018-06-29"
    ],
    "name": "StarGAN-VC",
    "source": "Arxiv",
    "title": "StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks",
    "url": "https://arxiv.org/abs/1806.02169"
  },
  {
    "abstract": "Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.",
    "authors": [
      "Wengling Chen",
      "James Hays"
    ],
    "dates": [
      "2018-01-09",
      "2018-04-12"
    ],
    "name": "SketchyGAN",
    "source": "CVPR 2018",
    "title": "SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis",
    "url": "https://arxiv.org/abs/1801.02753"
  },
  {
    "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been made available in this https URL.",
    "authors": [
      "Ping Luo",
      "Jiamin Ren",
      "Zhanglin Peng",
      "Ruimao Zhang",
      "Jingyu Li"
    ],
    "dates": [
      "2018-06-28",
      "2018-09-30"
    ],
    "name": "Switchable Normalization",
    "source": "Arxiv",
    "title": "Differentiable Learning-to-Normalize via Switchable Normalization",
    "url": "https://arxiv.org/abs/1806.10779"
  },
  {
    "abstract": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.",
    "authors": [
      "Yuxin Wu",
      "Kaiming He"
    ],
    "dates": [
      "2018-03-22",
      "2018-06-11"
    ],
    "name": "Group Normalization",
    "source": "Arxiv",
    "title": "Group Normalization",
    "url": "https://arxiv.org/abs/1803.08494"
  },
  {
    "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL",
    "authors": [
      "Diederik P. Kingma",
      "Prafulla Dhariwal"
    ],
    "dates": [
      "2018-07-09",
      "2018-07-10"
    ],
    "name": "Glow",
    "source": "Arxiv",
    "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
    "url": "https://arxiv.org/abs/1807.03039"
  },
  {
    "abstract": "Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.",
    "authors": [
      "Albert Pumarola",
      "Antonio Agudo",
      "Aleix M. Martinez",
      "Alberto Sanfeliu",
      "Francesc Moreno-Noguer"
    ],
    "dates": [
      "2018-07-24",
      "2018-08-28"
    ],
    "name": "GAnimation",
    "source": "ECCV 2018",
    "title": "GANimation: Anatomically-aware Facial Animation from a Single Image",
    "url": "https://arxiv.org/abs/1807.09251"
  },
  {
    "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at this http URL.",
    "authors": [
      "Ross Girshick",
      "Jeff Donahue",
      "Trevor Darrell",
      "Jitendra Malik"
    ],
    "dates": [
      "2013-11-11",
      "2014-10-22"
    ],
    "name": "R-CNN",
    "source": "CVPR 2014",
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
    "url": "https://arxiv.org/abs/1311.2524"
  },
  {
    "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at this https URL.",
    "authors": [
      "Ross Girshick"
    ],
    "dates": [
      "2015-04-30",
      "2015-09-27"
    ],
    "name": "Fast R-CNN",
    "source": "ICCV 2015",
    "title": "Fast R-CNN",
    "url": "https://arxiv.org/abs/1504.08083"
  },
  {
    "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
    "authors": [
      "Shaoqing Ren",
      "Kaiming He",
      "Ross Girshick",
      "Jian Sun"
    ],
    "dates": [
      "2015-06-04",
      "2016-01-06"
    ],
    "name": "Faster R-CNN",
    "source": "NIPS 2015",
    "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
    "url": "https://arxiv.org/abs/1506.01497"
  },
  {
    "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
    "authors": [
      "Joseph Redmon",
      "Santosh Divvala",
      "Ross Girshick",
      "Ali Farhadi"
    ],
    "dates": [
      "2015-06-08",
      "2016-05-09"
    ],
    "name": "YOLO",
    "source": "CVPR 2016",
    "title": "You Only Look Once: Unified, Real-Time Object Detection",
    "url": "https://arxiv.org/abs/1506.02640"
  },
  {
    "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.",
    "authors": [
      "Joseph Redmon",
      "Ali Farhadi"
    ],
    "dates": [
      "2016-12-25"
    ],
    "name": "YOLO9000",
    "source": "Arxiv",
    "title": "YOLO9000: Better, Faster, Stronger",
    "url": "https://arxiv.org/abs/1612.08242"
  },
  {
    "abstract": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",
    "authors": [
      "Joseph Redmon",
      "Ali Farhadi"
    ],
    "dates": [
      "2018-04-08"
    ],
    "name": "YOLOv3",
    "source": "Arxiv",
    "title": "YOLOv3: An Incremental Improvement",
    "url": "https://arxiv.org/abs/1804.02767"
  },
  {
    "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL .",
    "authors": [
      "Wei Liu",
      "Dragomir Anguelov",
      "Dumitru Erhan",
      "Christian Szegedy",
      "Scott Reed",
      "Cheng-Yang Fu",
      "Alexander C. Berg"
    ],
    "dates": [
      "2015-12-08",
      "2016-12-29"
    ],
    "name": "SSD",
    "source": "ECCV 2016",
    "title": "SSD: Single Shot MultiBox Detector",
    "url": "https://arxiv.org/abs/1512.02325"
  },
  {
    "abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",
    "authors": [
      "David Silver",
      "Aja Huang",
      "Chris J. Maddison",
      "Arthur Guez",
      "Laurent Sifre",
      "George van den Driessche",
      "Julian Schrittwieser",
      "Ioannis Antonoglou",
      "Veda Panneershelvam",
      "Marc Lanctot",
      "Sander Dieleman",
      "Dominik Grewe",
      "John Nham",
      "Nal Kalchbrenner",
      "Ilya Sutskever",
      "Timothy Lillicrap",
      "Madeleine Leach",
      "Koray Kavukcuoglu",
      "Thore Graepel",
      "Demis Hassabis"
    ],
    "dates": [
      "2016-01-27"
    ],
    "name": "AlphaGo",
    "source": "Nature 2016",
    "title": "Mastering the game of Go with deep neural networks and tree search",
    "url": "https://www.nature.com/articles/nature16961"
  },
  {
    "abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo\u2019s own move selections and also the winner of AlphaGo\u2019s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo.",
    "authors": [
      "David Silver",
      "Julian Schrittwieser",
      "Karen Simonyan",
      "Ioannis Antonoglou",
      "Aja Huang",
      "Arthur Guez",
      "Thomas Hubert",
      "Lucas Baker",
      "Matthew Lai",
      "Adrian Bolton",
      "Yutian Chen",
      "Timothy Lillicrap",
      "Fan Hui",
      "Laurent Sifre",
      "George van den Driessche",
      "Thore Graepel",
      "Demis Hassabis"
    ],
    "dates": [
      "2017-10-18"
    ],
    "name": "AlphaGo Zero",
    "source": "Nature 2017",
    "title": "Mastering the game of Go without human knowledge",
    "url": "https://www.nature.com/articles/nature24270"
  },
  {
    "abstract": "In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN) architecture has been proposed for speaker verification in the text-independent setting. One of the main challenges is the creation of the speaker models. Most of the previously-reported approaches create speaker models based on averaging the extracted features from utterances of the speaker, which is known as the d-vector system. In our paper, we propose an adaptive feature learning by utilizing the 3D-CNNs for direct speaker model creation in which, for both development and enrollment phases, an identical number of spoken utterances per speaker is fed to the network for representing the speakers' utterances and creation of the speaker model. This leads to simultaneously capturing the speaker-related information and building a more robust system to cope with within-speaker variation. We demonstrate that the proposed method significantly outperforms the traditional d-vector verification system. Moreover, the proposed system can also be an alternative to the traditional d-vector system which is a one-shot speaker modeling system by utilizing 3D-CNNs.",
    "authors": [
      "Amirsina Torfi",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "dates": [
      "2017-05-26",
      "2018-06-06"
    ],
    "name": "3DCNN (SV)",
    "source": "ICME 2018",
    "title": "Text-Independent Speaker Verification Using 3D Convolutional Neural Networks",
    "url": "https://arxiv.org/abs/1705.09422"
  },
  {
    "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
    "authors": [
      "Oriol Vinyals",
      "Charles Blundell",
      "Timothy Lillicrap",
      "Koray Kavukcuoglu",
      "Daan Wierstra"
    ],
    "dates": [
      "2016-06-13",
      "2017-12-29"
    ],
    "name": "Matching Networks",
    "source": "NIPS 2016",
    "title": "Matching Networks for One Shot Learning",
    "url": "https://arxiv.org/abs/1606.04080"
  },
  {
    "abstract": "We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.",
    "authors": [
      "Flood Sung",
      "Yongxin Yang",
      "Li Zhang",
      "Tao Xiang",
      "Philip H.S. Torr",
      "Timothy M. Hospedales"
    ],
    "dates": [
      "2017-11-16",
      "2018-03-27"
    ],
    "name": "Relation Network",
    "source": "CVPR 2018",
    "title": "Learning to Compare: Relation Network for Few-Shot Learning",
    "url": "https://arxiv.org/abs/1711.06025"
  },
  {
    "abstract": "Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.",
    "authors": [
      "Florian Schroff",
      "Dmitry Kalenichenko",
      "James Philbin"
    ],
    "dates": [
      "2015-03-12",
      "2015-06-17"
    ],
    "name": "Triplet Loss",
    "source": "CVPR 2015",
    "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
    "url": "https://arxiv.org/abs/1503.03832"
  },
  {
    "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
    "authors": [
      "Sara Sabour",
      "Nicholas Frosst",
      "Geoffrey E Hinton"
    ],
    "dates": [
      "2017-10-26",
      "2017-11-07"
    ],
    "name": "Capsules",
    "source": "NIPS 2017",
    "title": "Dynamic Routing Between Capsules",
    "url": "https://arxiv.org/abs/1710.09829"
  },
  {
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "authors": [
      "Diederik P. Kingma",
      "Jimmy Ba"
    ],
    "dates": [
      "2014-12-22",
      "2017-01-30"
    ],
    "name": "Adam",
    "source": "ICLR 2015",
    "title": "Adam: A Method for Stochastic Optimization",
    "url": "https://arxiv.org/abs/1412.6980"
  },
  {
    "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
    "authors": [
      "Vinod Nair",
      "Geoffrey E. Hinton"
    ],
    "dates": [
      "2010-06-21"
    ],
    "name": "ReLU",
    "source": "ICML 2010",
    "title": "Rectified linear units improve restricted boltzmann machines",
    "url": "https://dl.acm.org/citation.cfm?id=3104425"
  },
  {
    "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
    "authors": [
      "Kaiming He",
      "Xiangyu Zhang",
      "Shaoqing Ren",
      "Jian Sun"
    ],
    "dates": [
      "2015-02-06"
    ],
    "name": "PReLU",
    "source": "ICCV 2015",
    "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
    "url": "https://arxiv.org/abs/1502.01852"
  },
  {
    "abstract": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.",
    "authors": [
      "Bing Xu",
      "Naiyan Wang",
      "Tianqi Chen",
      "Mu Li"
    ],
    "dates": [
      "2015-05-05",
      "2015-11-27"
    ],
    "name": "RReLU",
    "source": "Arxiv",
    "title": "Empirical Evaluation of Rectified Activations in Convolutional Network",
    "url": "https://arxiv.org/abs/1505.00853"
  },
  {
    "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
    "authors": [
      "Djork-Arn\u00e9 Clevert",
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "dates": [
      "2015-11-23",
      "2016-02-22"
    ],
    "name": "ELU",
    "source": "ICLR 2016",
    "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
    "url": "https://arxiv.org/abs/1511.07289"
  },
  {
    "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",
    "authors": [
      "G\u00fcnter Klambauer",
      "Thomas Unterthiner",
      "Andreas Mayr",
      "Sepp Hochreiter"
    ],
    "dates": [
      "2017-06-08",
      "2017-09-07"
    ],
    "name": "SNN",
    "source": "NIPS 2017",
    "title": "Self-Normalizing Neural Networks",
    "url": "https://arxiv.org/abs/1706.02515"
  },
  {
    "abstract": "Self-attentive feed-forward sequence models have been shown to achieve impressive results on sequence modeling tasks, thereby presenting a compelling alternative to recurrent neural networks (RNNs) which has remained the de-facto standard architecture for many sequence modeling problems to date. Despite these successes, however, feed-forward sequence models like the Transformer fail to generalize in many tasks that recurrent models handle with ease (e.g. copying when the string lengths exceed those observed at training time). Moreover, and in contrast to RNNs, the Transformer model is not computationally universal, limiting its theoretical expressivity. In this paper we propose the Universal Transformer which addresses these practical and theoretical shortcomings and we show that it leads to improved performance on several tasks. Instead of recurring over the individual symbols of sequences like RNNs, the Universal Transformer repeatedly revises its representations of all symbols in the sequence with each recurrent step. In order to combine information from different parts of a sequence, it employs a self-attention mechanism in every recurrent step. Assuming sufficient memory, its recurrence makes the Universal Transformer computationally universal. We further employ an adaptive computation time (ACT) mechanism to allow the model to dynamically adjust the number of times the representation of each position in a sequence is revised. Beyond saving computation, we show that ACT can improve the accuracy of the model. Our experiments show that on various algorithmic tasks and a diverse set of large-scale language understanding tasks the Universal Transformer generalizes significantly better and outperforms both a vanilla Transformer and an LSTM in machine translation, and achieves a new state of the art on the bAbI linguistic reasoning task and the challenging LAMBADA language modeling task.",
    "authors": [
      "Mostafa Dehghani",
      "Stephan Gouws",
      "Oriol Vinyals",
      "Jakob Uszkoreit",
      "\u0141ukasz Kaiser"
    ],
    "dates": [
      "2018-07-10"
    ],
    "name": "Universal Transformer",
    "source": "Arxiv",
    "title": "Universal Transformers",
    "url": "https://arxiv.org/abs/1807.03819"
  },
  {
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "authors": [
      "Matthew E. Peters",
      "Mark Neumann",
      "Mohit Iyyer",
      "Matt Gardner",
      "Christopher Clark",
      "Kenton Lee",
      "Luke Zettlemoyer"
    ],
    "dates": [
      "2018-02-15",
      "2018-03-22"
    ],
    "name": "ELMo",
    "source": "NAACL 2018",
    "title": "Deep contextualized word representations",
    "url": "https://arxiv.org/abs/1802.05365"
  },
  {
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",
    "authors": [
      "Jeremy Howard",
      "Sebastian Ruder"
    ],
    "dates": [
      "2018-01-18",
      "2018-05-23"
    ],
    "name": "ULMFiT",
    "source": "ACL 2018",
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "url": "https://arxiv.org/abs/1801.06146"
  },
  {
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "dates": [
      "2018-10-11"
    ],
    "name": "BERT",
    "source": "Arxiv",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "url": "https://arxiv.org/abs/1810.04805"
  },
  {
    "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.",
    "authors": [
      "Chao Dong",
      "Chen Change Loy",
      "Kaiming He",
      "Xiaoou Tang"
    ],
    "dates": [
      "2014-12-31",
      "2015-07-31"
    ],
    "name": "SRCNN",
    "source": "ECCV 2014",
    "title": "Image Super-Resolution Using Deep Convolutional Networks",
    "url": "https://arxiv.org/abs/1501.00092"
  },
  {
    "abstract": "As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.",
    "authors": [
      "Chao Dong",
      "Chen Change Loy",
      "Xiaoou Tang"
    ],
    "dates": [
      "2016-08-01"
    ],
    "name": "FSRCNN",
    "source": "ECCV 2016",
    "title": "Accelerating the Super-Resolution Convolutional Neural Network",
    "url": "https://arxiv.org/abs/1608.00367"
  },
  {
    "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
    "authors": [
      "Wenzhe Shi",
      "Jose Caballero",
      "Ferenc Husz\u00e1r",
      "Johannes Totz",
      "Andrew P. Aitken",
      "Rob Bishop",
      "Daniel Rueckert",
      "Zehan Wang"
    ],
    "dates": [
      "2016-09-16",
      "2016-09-23"
    ],
    "name": "ESPCN",
    "source": "CVPR 2016",
    "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
    "url": "https://arxiv.org/abs/1609.05158"
  },
  {
    "abstract": "We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification \\cite{simonyan2015very}. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates ($10^4$ times higher than SRCNN \\cite{dong2015image}) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.",
    "authors": [
      "Jiwon Kim",
      "Jung Kwon Lee",
      "Kyoung Mu Lee"
    ],
    "dates": [
      "2015-11-14",
      "2016-11-11"
    ],
    "name": "VDSR",
    "source": "CVPR 2016",
    "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks",
    "url": "https://arxiv.org/abs/1511.04587"
  },
  {
    "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.",
    "authors": [
      "Jiwon Kim",
      "Jung Kwon Lee",
      "Kyoung Mu Lee"
    ],
    "dates": [
      "2015-11-14",
      "2016-11-11"
    ],
    "name": "DRCN",
    "source": "CVPR 2016",
    "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
    "url": "https://arxiv.org/abs/1511.04491"
  },
  {
    "abstract": "Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.",
    "authors": [
      "Xiao-Jiao Mao",
      "Chunhua Shen",
      "Yu-Bin Yang"
    ],
    "dates": [
      "2016-06-29",
      "2016-08-30"
    ],
    "name": "RED",
    "source": "NIPS 2016",
    "title": "Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections",
    "url": "https://arxiv.org/abs/1606.08921"
  },
  {
    "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.",
    "authors": [
      "Ji He",
      "Jianshu Chen",
      "Xiaodong He",
      "Jianfeng Gao",
      "Lihong Li",
      "Li Deng",
      "Mari Ostendorf"
    ],
    "dates": [
      "2015-11-14",
      "2016-06-08"
    ],
    "name": "DRRN",
    "source": "ACL 2016",
    "title": "Deep Reinforcement Learning with a Natural Language Action Space",
    "url": "https://arxiv.org/abs/1511.04636"
  },
  {
    "abstract": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",
    "authors": [
      "Wei-Sheng Lai",
      "Jia-Bin Huang",
      "Narendra Ahuja",
      "Ming-Hsuan Yang"
    ],
    "dates": [
      "2017-04-12",
      "2017-10-09"
    ],
    "name": "LapSRN",
    "source": "CVPR 2017",
    "title": "Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution",
    "url": "https://arxiv.org/abs/1704.03915"
  },
  {
    "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
    "authors": [
      "Christian Ledig",
      "Lucas Theis",
      "Ferenc Huszar",
      "Jose Caballero",
      "Andrew Cunningham",
      "Alejandro Acosta",
      "Andrew Aitken",
      "Alykhan Tejani",
      "Johannes Totz",
      "Zehan Wang",
      "Wenzhe Shi"
    ],
    "dates": [
      "2016-09-15",
      "2017-05-25"
    ],
    "name": "SRGAN",
    "source": "CVPR 2017",
    "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
    "url": "https://arxiv.org/abs/1609.04802"
  },
  {
    "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
    "authors": [
      "Christian Ledig",
      "Lucas Theis",
      "Ferenc Huszar",
      "Jose Caballero",
      "Andrew Cunningham",
      "Alejandro Acosta",
      "Andrew Aitken",
      "Alykhan Tejani",
      "Johannes Totz",
      "Zehan Wang",
      "Wenzhe Shi"
    ],
    "dates": [
      "2016-09-15",
      "2017-05-25"
    ],
    "name": "SRResNet",
    "source": "CVPR 2017",
    "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
    "url": "https://arxiv.org/abs/1609.04802"
  },
  {
    "abstract": "Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.",
    "authors": [
      "Bee Lim",
      "Sanghyun Son",
      "Heewon Kim",
      "Seungjun Nah",
      "Kyoung Mu Lee"
    ],
    "dates": [
      "2017-07-10"
    ],
    "name": "EDSR",
    "source": "CVPRW 2017",
    "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
    "url": "https://arxiv.org/abs/1707.02921"
  },
  {
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick\", allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "authors": [
      "Andrew Brock",
      "Jeff Donahue",
      "Karen Simonyan"
    ],
    "dates": [
      "2018-09-28"
    ],
    "name": "BigGAN",
    "source": "ICLR 2019",
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "url": "https://arxiv.org/abs/1809.11096"
  },
  {
    "abstract": "Adversarial training has been highly successful in the context of image super-resolution. It was demonstrated to yield realistic and highly detailed results. Despite this success, many state-of-the-art methods for video super-resolution still favor simpler norms such as $L_2$ over adversarial loss functions. This is caused by the fact that the averaging nature of direct vector norms as loss functions leads to temporal smoothness. The lack of spatial detail means temporal coherence is easily established. In our work, we instead propose an adversarial training for video super-resolution that leads to temporally coherent solutions without sacrificing spatial detail. In our generator, we use a recurrent, residual framework that naturally encourages temporal consistency. For adversarial training, we propose a novel spatio-temporal discriminator in combination with motion compensation to guarantee photo-realistic and temporally coherent details in the results. We additionally identify a class of temporal artifacts in these recurrent networks, and propose a novel Ping-Pong loss to remove them. Quantifying the temporal coherence for image super-resolution tasks has also not been addressed previously. We propose a first set of metrics to evaluate the accuracy as well as the perceptual quality of the temporal evolution, and we demonstrate that our method outperforms previous work by yielding realistic and detailed images with natural temporal changes.",
    "authors": [
      "Mengyu Chu",
      "You Xie",
      "Laura Leal-Taix\u00e9",
      "Nils Thuerey"
    ],
    "dates": [
      "2018-11-23"
    ],
    "name": "TecoGAN",
    "source": "Arxiv",
    "title": "Temporally Coherent GANs for Video Super-Resolution (TecoGAN)",
    "url": "https://arxiv.org/abs/1811.09393"
  },
  {
    "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
    "authors": [
      "Sepp Hochreiter",
      "J\u00fcrgen Schmidhuber"
    ],
    "dates": [
      "Long"
    ],
    "name": "LSTM",
    "source": "Journal Neural Computation Volume 9 Issue 8",
    "title": "Long Short-Term Memory",
    "url": "https://dl.acm.org/citation.cfm?id=1246450"
  },
  {
    "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
    "authors": [
      "Kyunghyun Cho",
      "Bart van Merrienboer",
      "Dzmitry Bahdanau",
      "Yoshua Bengio"
    ],
    "dates": [
      "2014-09-03",
      "2014-10-07"
    ],
    "name": "GRU",
    "source": "SSST-8",
    "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
    "url": "https://arxiv.org/abs/1409.1259"
  },
  {
    "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
    "authors": [
      "Tero Karras",
      "Samuli Laine",
      "Timo Aila"
    ],
    "dates": [
      "2018-12-12",
      "2019-02-06"
    ],
    "name": "StyleGAN",
    "source": "Arxiv",
    "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1812.04948"
  },
  {
    "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
    "authors": [
      "Jimmy Lei Ba",
      "Jamie Ryan Kiros",
      "Geoffrey E. Hinton"
    ],
    "dates": [
      "2016-07-21"
    ],
    "name": "Layer Normalization",
    "source": "Arxiv",
    "title": "Layer Normalization",
    "url": "https://arxiv.org/abs/1607.06450"
  },
  {
    "abstract": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
    "authors": [
      "Vincent Dumoulin",
      "Jonathon Shlens",
      "Manjunath Kudlur"
    ],
    "dates": [
      "2016-10-24",
      "2017-02-09"
    ],
    "name": "Conditional Layer Normalization",
    "source": "ICLR 2017",
    "title": "A Learned Representation For Artistic Style",
    "url": "https://arxiv.org/abs/1610.07629"
  },
  {
    "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",
    "authors": [
      "Xun Huang",
      "Serge Belongie"
    ],
    "dates": [
      "2017-03-20",
      "2017-07-30"
    ],
    "name": "Adaptive Instance Normalization",
    "source": "ICCV 2017",
    "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
    "url": "https://arxiv.org/abs/1703.06868"
  },
  {
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
    "authors": [
      "Tero Karras",
      "Timo Aila",
      "Samuli Laine",
      "Jaakko Lehtinen"
    ],
    "dates": [
      "2017-10-27",
      "2018-02-26"
    ],
    "name": "Progressive GAN",
    "source": "ICLR 2018",
    "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
    "url": "https://arxiv.org/abs/1710.10196"
  },
  {
    "abstract": "We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.",
    "authors": [
      "Youngjoo Jo",
      "Jongyoul Park"
    ],
    "dates": [
      "2019-02-18"
    ],
    "name": "SC-FEGAN",
    "source": "Arxiv",
    "title": "SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color",
    "url": "https://arxiv.org/abs/1902.06838"
  },
  {
    "abstract": "Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-sequence models. Lingvo models are composed of modular building blocks that are flexible and easily extensible, and experiment configurations are centralized and highly customizable. Distributed training and quantized inference are supported directly within the framework, and it contains existing implementations of a large number of utilities, helper functions, and the newest research ideas. Lingvo has been used in collaboration by dozens of researchers in more than 20 papers over the last two years. This document outlines the underlying design of Lingvo and serves as an introduction to the various pieces of the framework, while also offering examples of advanced features that showcase the capabilities of the framework.",
    "authors": [
      "Jonathan Shen",
      "Patrick Nguyen",
      "Yonghui Wu",
      "Zhifeng Chen",
      "Mia X. Chen",
      "Ye Jia",
      "Anjuli Kannan",
      "Tara Sainath",
      "Yuan Cao",
      "Chung-Cheng Chiu",
      "Yanzhang He",
      "Jan Chorowski",
      "Smit Hinsu",
      "Stella Laurenzo",
      "James Qin",
      "Orhan Firat",
      "Wolfgang Macherey",
      "Suyog Gupta",
      "Ankur Bapna",
      "Shuyuan Zhang",
      "Ruoming Pang",
      "Ron J. Weiss",
      "Rohit Prabhavalkar",
      "Qiao Liang",
      "Benoit Jacob",
      "Bowen Liang",
      "HyoukJoong Lee",
      "Ciprian Chelba",
      "S\u00e9bastien Jean",
      "Bo Li",
      "Melvin Johnson",
      "Rohan Anil",
      "Rajat Tibrewal",
      "Xiaobing Liu",
      "Akiko Eriguchi",
      "Navdeep Jaitly",
      "Naveen Ari",
      "Colin Cherry",
      "Parisa Haghani",
      "Otavio Good",
      "Youlong Cheng",
      "Raziel Alvarez",
      "Isaac Caswell",
      "Wei-Ning Hsu",
      "Zongheng Yang",
      "Kuan-Chieh Wang",
      "Ekaterina Gonina",
      "Katrin Tomanek",
      "Ben Vanik",
      "Zelin Wu",
      "Llion Jones",
      "Mike Schuster",
      "Yanping Huang",
      "Dehao Chen",
      "Kazuki Irie",
      "George Foster",
      "John Richardson",
      "Klaus Macherey",
      "Antoine Bruguier",
      "Heiga Zen",
      "Colin Raffel",
      "Shankar Kumar",
      "Kanishka Rao",
      "David Rybach",
      "Matthew Murray",
      "Vijayaditya Peddinti",
      "Maxim Krikun",
      "Michiel A. U. Bacchiani",
      "Thomas B. Jablin",
      "Rob Suderman",
      "Ian Williams",
      "Benjamin Lee",
      "Deepti Bhatia",
      "Justin Carlson",
      "Semih Yavuz",
      "Yu Zhang",
      "Ian McGraw",
      "Max Galkin",
      "Qi Ge",
      "Golan Pundak",
      "Chad Whipkey",
      "Todd Wang",
      "Uri Alon",
      "Dmitry Lepikhin",
      "Ye Tian",
      "Sara Sabour",
      "William Chan",
      "Shubham Toshniwal",
      "Baohua Liao",
      "Michael Nirschl",
      "Pat Rondon"
    ],
    "dates": [
      "2019-02-21"
    ],
    "name": "Lingvo",
    "source": "Arxiv",
    "title": "Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling",
    "url": "https://arxiv.org/abs/1902.08295"
  },
  {
    "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is this http URL.",
    "authors": [
      "Qiang Wang",
      "Li Zhang",
      "Luca Bertinetto",
      "Weiming Hu",
      "Philip H.S. Torr"
    ],
    "dates": [
      "2018-12-12"
    ],
    "name": "SiamMask",
    "source": "CVPR 2019",
    "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach",
    "url": "https://arxiv.org/abs/1812.05050"
  },
  {
    "abstract": "Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms existing methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",
    "authors": [
      "Xin Wang",
      "Qiuyuan Huang",
      "Asli Celikyilmaz",
      "Jianfeng Gao",
      "Dinghan Shen",
      "Yuan-Fang Wang",
      "William Yang Wang",
      "Lei Zhang"
    ],
    "dates": [
      "2018-11-25"
    ],
    "name": "Reinforced Cross-Modal Matching",
    "source": "CVPR 2019",
    "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation",
    "url": "https://arxiv.org/abs/1811.10092"
  },
  {
    "abstract": "Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at \\url{this https URL}.",
    "authors": [
      "Zhaojin Huang",
      "Lichao Huang",
      "Yongchao Gong",
      "Chang Huang",
      "Xinggang Wang"
    ],
    "dates": [
      "2019-03-01"
    ],
    "name": "Mask Scoring R-CNN",
    "source": "CVPR 2019",
    "title": "Mask Scoring R-CNN",
    "url": "https://arxiv.org/abs/1903.00241"
  },
  {
    "abstract": "Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labelled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, to close the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 33 on unconditional ImageNet generation.",
    "authors": [
      "Ting Chen",
      "Xiaohua Zhai",
      "Marvin Ritter",
      "Mario Lucic",
      "Neil Houlsby"
    ],
    "dates": [
      "2018-11-27"
    ],
    "name": "Self-Supervised GAN",
    "source": "Arxiv",
    "title": "Self-Supervised Generative Adversarial Networks",
    "url": "https://arxiv.org/abs/1811.11212"
  },
  {
    "abstract": "Recent research on super-resolution has achieved great success due to the development of deep convolutional neural networks (DCNNs). However, super-resolution of arbitrary scale factor has been ignored for a long time. Most previous researchers regard super-resolution of different scale factors as independent tasks. They train a specific model for each scale factor which is inefficient in computing, and prior work only take the super-resolution of several integer scale factors into consideration. In this work, we propose a novel method called Meta-SR to firstly solve super-resolution of arbitrary scale factor (including non-integer scale factors) with a single model. In our Meta-SR, the Meta-Upscale Module is proposed to replace the traditional upscale module. For arbitrary scale factor, the Meta-Upscale Module dynamically predicts the weights of the upscale filters by taking the scale factor as input and use these weights to generate the HR image of arbitrary size. For any low-resolution image, our Meta-SR can continuously zoom in it with arbitrary scale factor by only using a single model. We evaluated the proposed method through extensive experiments on widely used benchmark datasets on single image super-resolution. The experimental results show the superiority of our Meta-Upscale.",
    "authors": [
      "Xuecai Hu",
      "Haoyuan Mu",
      "Xiangyu Zhang",
      "Zilei Wang",
      "Jian Sun",
      "Tieniu Tan"
    ],
    "dates": [
      "2019-03-03"
    ],
    "name": "Meta-SR",
    "source": "CVPR 2019",
    "title": "Meta-SR: A Magnification-Arbitrary Network for Super-Resolution",
    "url": "https://arxiv.org/abs/1903.00875"
  },
  {
    "abstract": "Graphics Interchange Format (GIF) is a highly portable graphics format that is ubiquitous on the Internet. Despite their small sizes, GIF images often contain undesirable visual artifacts such as flat color regions, false contours, color shift, and dotted patterns. In this paper, we propose GIF2Video, the first learning-based method for enhancing the visual quality of GIFs in the wild. We focus on the challenging task of GIF restoration by recovering information lost in the three steps of GIF creation: frame sampling, color quantization, and color dithering. We first propose a novel CNN architecture for color dequantization. It is built upon a compositional architecture for multi-step color correction, with a comprehensive loss function designed to handle large quantization errors. We then adapt the SuperSlomo network for temporal interpolation of GIF frames. We introduce two large datasets, namely GIF-Faces and GIF-Moments, for both training and evaluation. Experimental results show that our method can significantly improve the visual quality of GIFs, and outperforms direct baseline and state-of-the-art approaches.",
    "authors": [
      "Yang Wang",
      "Haibin Huang",
      "Chuan Wang",
      "Tong He",
      "Jue Wang",
      "Minh Hoai"
    ],
    "dates": [
      "2019-01-09"
    ],
    "name": "GIF2Video",
    "source": "CVPR 2019",
    "title": "GIF2Video: Color Dequantization and Temporal Interpolation of GIF images",
    "url": "https://arxiv.org/abs/1901.02840"
  },
  {
    "abstract": "Surface-based geodesic topology provides strong cues for object semantic analysis and geometric modeling. However, such connectivity information is lost in point clouds. Thus we introduce GeoNet, the first deep learning architecture trained to model the intrinsic structure of surfaces represented as point clouds. To demonstrate the applicability of learned geodesic-aware representations, we propose fusion schemes which use GeoNet in conjunction with other baseline or backbone networks, such as PU-Net and PointNet++, for down-stream point cloud analysis. Our method improves the state-of-the-art on multiple representative tasks that can benefit from understandings of the underlying surface topology, including point upsampling, normal estimation, mesh reconstruction and non-rigid shape classification.",
    "authors": [
      "Tong He",
      "Haibin Huang",
      "Li Yi",
      "Yuqian Zhou",
      "Chihao Wu",
      "Jue Wang",
      "Stefano Soatto"
    ],
    "dates": [
      "2019-01-03"
    ],
    "name": "GeoNet",
    "source": "CVPR 2019",
    "title": "GeoNet: Deep Geodesic Networks for Point Cloud Analysis",
    "url": "https://arxiv.org/abs/1901.00680"
  },
  {
    "abstract": "The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.",
    "authors": [
      "Rudra P K Poudel",
      "Stephan Liwicki",
      "Roberto Cipolla"
    ],
    "dates": [
      "2019-02-12"
    ],
    "name": "Fast-SCNN",
    "source": "Arxiv",
    "title": "Fast-SCNN: Fast Semantic Segmentation Network",
    "url": "https://arxiv.org/abs/1902.04502"
  },
  {
    "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",
    "authors": [
      "Ting-Chun Wang",
      "Ming-Yu Liu",
      "Jun-Yan Zhu",
      "Andrew Tao",
      "Jan Kautz",
      "Bryan Catanzaro"
    ],
    "dates": [
      "2017-11-30",
      "2018-08-20"
    ],
    "name": "pix2pixHD",
    "source": "CVPR 2018",
    "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
    "url": "https://arxiv.org/abs/1711.11585"
  },
  {
    "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style as synthesizing images. Code will be available at this https URL .",
    "authors": [
      "Taesung Park",
      "Ming-Yu Liu",
      "Ting-Chun Wang",
      "Jun-Yan Zhu"
    ],
    "dates": [
      "2019-03-18"
    ],
    "name": "SPADE",
    "source": "CVPR 2019",
    "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
    "url": "https://arxiv.org/abs/1903.07291"
  },
  {
    "abstract": "Text effects transfer technology automatically makes the text dramatically more impressive. However, previous style transfer methods either study the model for general style, which cannot handle the highly-structured text effects along the glyph, or require manual design of subtle matching criteria for text effects. In this paper, we focus on the use of the powerful representation abilities of deep neural features for text effects transfer. For this purpose, we propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a stylization subnetwork and a destylization subnetwork. The key idea is to train our network to accomplish both the objective of style transfer and style removal, so that it can learn to disentangle and recombine the content and style features of text effects images. To support the training of our network, we propose a new text effects dataset with as much as 64 professionally designed styles on 837 characters. We show that the disentangled feature representations enable us to transfer or remove all these styles on arbitrary glyphs using one network. Furthermore, the flexible network design empowers TET-GAN to efficiently extend to a new text style via one-shot learning where only one example is required. We demonstrate the superiority of the proposed method in generating high-quality stylized text over the state-of-the-art methods.",
    "authors": [
      "Shuai Yang",
      "Jiaying Liu",
      "Wenjing Wang",
      "Zongming Guo"
    ],
    "dates": [
      "2018-12-16",
      "2018-12-27"
    ],
    "name": "TET-GAN",
    "source": "AAAI 2019",
    "title": "TET-GAN: Text Effects Transfer via Stylization and Destylization",
    "url": "https://arxiv.org/abs/1812.06384"
  },
  {
    "abstract": "We propose a universal image reconstruction method to represent detailed images purely from binary sparse edge and flat color domain. Inspired by the procedures of painting, our framework, based on generative adversarial network, consists of three phases: Imitation Phase aims at initializing networks, followed by Generating Phase to reconstruct preliminary images. Moreover, Refinement Phase is utilized to fine-tune preliminary images into final outputs with details. This framework allows our model generating abundant high frequency details from sparse input information. We also explore the defects of disentangling style latent space implicitly from images, and demonstrate that explicit color domain in our model performs better on controllability and interpretability. In our experiments, we achieve outstanding results on reconstructing realistic images and translating hand drawn drafts into satisfactory paintings. Besides, within the domain of edge-to-image translation, our model PI-REC outperforms existing state-of-the-art methods on evaluations of realism and accuracy, both quantitatively and qualitatively.",
    "authors": [
      "Sheng You",
      "Ning You",
      "Minxue Pan"
    ],
    "dates": [
      "2019-03-25"
    ],
    "name": "PI-REC",
    "source": "Arxiv",
    "title": "PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain",
    "url": "https://arxiv.org/abs/1903.10146"
  },
  {
    "abstract": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.",
    "authors": [
      "Patsorn Sangkloy",
      "Jingwan Lu",
      "Chen Fang",
      "Fisher Yu",
      "James Hays"
    ],
    "dates": [
      "2016-12-02",
      "2016-12-05"
    ],
    "name": "Scribbler",
    "source": "CVPR 2017",
    "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color",
    "url": "https://arxiv.org/abs/1612.00835"
  },
  {
    "abstract": "Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.",
    "authors": [
      "Wengling Chen",
      "James Hays"
    ],
    "dates": [
      "2018-01-09",
      "2018-04-12"
    ],
    "name": "SketchyGAN",
    "source": "CVPR 2018",
    "title": "SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis",
    "url": "https://arxiv.org/abs/1801.02753"
  },
  {
    "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
    "authors": [
      "Jun-Yan Zhu",
      "Richard Zhang",
      "Deepak Pathak",
      "Trevor Darrell",
      "Alexei A. Efros",
      "Oliver Wang",
      "Eli Shechtman"
    ],
    "dates": [
      "2017-11-30",
      "2018-10-24"
    ],
    "name": "BicycleGAN",
    "source": "NIPS 2017",
    "title": "Toward Multimodal Image-to-Image Translation",
    "url": "https://arxiv.org/abs/1711.11586"
  },
  {
    "abstract": "This paper presents a simple method for \"do as I do\" motion transfer: given a source video of a person dancing we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We pose this problem as a per-frame image-to-image translation with spatio-temporal smoothing. Using pose detections as an intermediate representation between source and target, we learn a mapping from pose images to a target subject's appearance. We adapt this setup for temporally coherent video generation including realistic face synthesis. Our video demo can be found at this https URL .",
    "authors": [
      "Caroline Chan",
      "Shiry Ginosar",
      "Tinghui Zhou",
      "Alexei A. Efros"
    ],
    "dates": [
      "2018-08-22"
    ],
    "name": "Everybody Dance Now",
    "source": "Arxiv",
    "title": "Everybody Dance Now",
    "url": "https://arxiv.org/abs/1808.07371"
  },
  {
    "abstract": "Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: this https URL.",
    "authors": [
      "Jiahui Yu",
      "Zhe Lin",
      "Jimei Yang",
      "Xiaohui Shen",
      "Xin Lu",
      "Thomas S. Huang"
    ],
    "dates": [
      "2018-01-24",
      "2018-03-21"
    ],
    "name": "Generative Image Inpainting with Contextual Attention",
    "source": "CVPR 2018",
    "title": "Generative Image Inpainting with Contextual Attention",
    "url": "https://arxiv.org/abs/1801.07892"
  },
  {
    "abstract": "In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",
    "authors": [
      "Aliaksandr Siarohin",
      "Enver Sangineto",
      "Stephane Lathuiliere",
      "Nicu Sebe"
    ],
    "dates": [
      "2017-12-29",
      "2018-04-06"
    ],
    "name": "PoseGAN",
    "source": "CVPR 2018",
    "title": "Deformable GANs for Pose-based Human Image Generation",
    "url": "https://arxiv.org/abs/1801.00055"
  },
  {
    "abstract": "Graph embedding methods produce unsupervised node features from graphs that can then be used for a variety of machine learning tasks. Modern graphs, particularly in industrial applications, contain billions of nodes and trillions of edges, which exceeds the capability of existing embedding systems. We present PyTorch-BigGraph (PBG), an embedding system that incorporates several modifications to traditional multi-relation embedding systems that allow it to scale to graphs with billions of nodes and trillions of edges. PBG uses graph partitioning to train arbitrarily large embeddings on either a single machine or in a distributed environment. We demonstrate comparable performance with existing embedding systems on common benchmarks, while allowing for scaling to arbitrarily large graphs and parallelization on multiple machines. We train and evaluate embeddings on several large social network graphs as well as the full Freebase dataset, which contains over 100 million nodes and 2 billion edges.",
    "authors": [
      "Adam Lerer",
      "Ledell Wu",
      "Jiajun Shen",
      "Timothee Lacroix",
      "Luca Wehrstedt",
      "Abhijit Bose",
      "Alex Peysakhovich"
    ],
    "dates": [
      "2019-03-28",
      "2019-04-01"
    ],
    "name": "PyTorch-BigGraph",
    "source": "Arxiv",
    "title": "PyTorch-BigGraph: A Large-scale Graph Embedding System",
    "url": "https://arxiv.org/abs/1903.12287"
  },
  {
    "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
    "authors": [
      "Kaiming He",
      "Georgia Gkioxari",
      "Piotr Doll\u00e1r",
      "Ross Girshick"
    ],
    "dates": [
      "2017-03-20",
      "2018-01-24"
    ],
    "name": "Mask R-CNN",
    "source": "ICCV 2017",
    "title": "Mask R-CNN",
    "url": "https://arxiv.org/abs/1703.06870"
  },
  {
    "abstract": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL .",
    "authors": [
      "Ming-Yu Liu",
      "Thomas Breuel",
      "Jan Kautz"
    ],
    "dates": [
      "2017-03-02",
      "2018-07-23"
    ],
    "name": "UNIT",
    "source": "NIPS 2017",
    "title": "Unsupervised Image-to-Image Translation Networks",
    "url": "https://arxiv.org/abs/1703.00848"
  },
  {
    "abstract": "Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at this https URL",
    "authors": [
      "Xun Huang",
      "Ming-Yu Liu",
      "Serge Belongie",
      "Jan Kautz"
    ],
    "dates": [
      "2018-04-12",
      "2018-08-14"
    ],
    "name": "MUNIT",
    "source": "ECCV 2018",
    "title": "Multimodal Unsupervised Image-to-Image Translation",
    "url": "https://arxiv.org/abs/1804.04732"
  },
  {
    "abstract": "Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and the attribute vectors sampled from the attribute space to produce diverse outputs at test time. To handle unpaired training data, we introduce a novel cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative comparisons, we measure realism with user study and diversity with a perceptual distance metric. We apply the proposed model to domain adaptation and show competitive performance when compared to the state-of-the-art on the MNIST-M and the LineMod datasets.",
    "authors": [
      "Hsin-Ying Lee",
      "Hung-Yu Tseng",
      "Jia-Bin Huang",
      "Maneesh Kumar Singh",
      "Ming-Hsuan Yang"
    ],
    "dates": [
      "2018-08-02"
    ],
    "name": "DRIT",
    "source": "ECCV 2018",
    "title": "Diverse Image-to-Image Translation via Disentangled Representations",
    "url": "https://arxiv.org/abs/1808.00948"
  }
]